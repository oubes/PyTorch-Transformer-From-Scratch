{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d007a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da749f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341bc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    InputEmbeddings class:\n",
    "    - Converts input token indices into dense vectors of dimension `d_model`.\n",
    "    - Scales embeddings by sqrt(d_model) to stabilize gradients in Transformer models.\n",
    "    - Parameters:\n",
    "        - d_model (int): size of each embedding vector.\n",
    "        - num_embeddings (int): size of the vocabulary.\n",
    "    - Usage: feeds into Transformer encoder/decoder as input embeddings.\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length), dtype=torch.long containing token indices.\n",
    "    Outputs:\n",
    "        - embeddings (Tensor): shape (batch_size, seq_length, d_model) scaled embeddings ready for the Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_embeddings: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of Vec\n",
    "        self.num_embeddings = num_embeddings # Size of Vocab\n",
    "        self.embedding = nn.Embedding(num_embeddings, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbf59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding class:\n",
    "    - Adds positional information to token embeddings so the Transformer can capture the order of tokens.\n",
    "    - Uses sine and cosine functions of different frequencies for each dimension.\n",
    "    - Parameters:\n",
    "        - d_model (int): dimension of embedding vectors.\n",
    "        - seq_len (int): maximum sequence length.\n",
    "        - dropout (float): dropout rate applied after adding positional encoding.\n",
    "    - Usage: added to token embeddings before feeding into Transformer layers.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model) token embeddings from InputEmbeddings.\n",
    "    Outputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model) token embeddings with positional information added, dropout applied.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        i = torch.arange(0, d_model, 2, dtype=torch.float)\n",
    "        div_term = torch.exp(i * (-math.log(10000)) / d_model)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :])\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da09f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNormalization class:\n",
    "    - Normalizes inputs across the last dimension to have zero mean and unit variance.\n",
    "    - Learnable parameters (alpha, bias) allow the model to scale and shift the normalized values.\n",
    "    - Helps stabilize and accelerate training of deep networks, especially Transformers.\n",
    "    - Parameters:\n",
    "        - eps (float): small value to avoid division by zero (default 1e-6).\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (..., features), can be any shape with the last dim as features.\n",
    "    Outputs:\n",
    "        - normalized_x (Tensor): same shape as input, normalized along the last dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SubLayer(nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    SubLayer abstract class:\n",
    "    - Base class for Transformer sub-layers (e.g., attention, feed-forward).\n",
    "    - Enforces implementation of the `forward` method in all subclasses.\n",
    "    - Can accept additional keyword arguments for flexibility.\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input embeddings or outputs from previous layer.\n",
    "        - **kwargs: optional additional arguments needed by specific sub-layers.\n",
    "    Outputs:\n",
    "        - Tensor: processed output, same shape as input in most cases.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def forward(self, x, **kwargs):\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(SubLayer):\n",
    "    \"\"\"\n",
    "    FeedForward sub-layer for Transformers:\n",
    "    - Implements a 2-layer position-wise feed-forward network.\n",
    "    - Applies ReLU activation and dropout between the two linear layers.\n",
    "    - Expands and then projects back to `d_model` dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "        - d_model (int): input and output dimension of the sub-layer.\n",
    "        - d_ff (int): hidden layer dimension (usually larger than d_model).\n",
    "        - dropout (float): dropout probability applied after the activation.\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input from previous layer.\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), transformed output.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279dbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    ResidualConnection module for Transformers:\n",
    "    - Wraps a sub-layer (e.g., FeedForward or Attention) with a residual connection and layer normalization.\n",
    "    - Supports both Pre-Norm and Post-Norm variants:\n",
    "        * Pre-Norm: normalization before the sub-layer.\n",
    "        * Post-Norm: normalization after adding the sub-layer output.\n",
    "    - Applies dropout to the sub-layer output before adding the residual.\n",
    "\n",
    "    Parameters:\n",
    "        - dropout (float): dropout probability applied to sub-layer output.\n",
    "        - pre_norm (bool): if True, use Pre-Norm; else, use Post-Norm.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input to the residual block.\n",
    "        - sub_layer (SubLayer): a Transformer sub-layer implementing the forward(x) method.\n",
    "\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), output after residual addition and normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout: float, pre_norm: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.pre_norm = pre_norm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sub_layer: SubLayer):\n",
    "        if self.pre_norm:\n",
    "            return x + self.dropout(sub_layer(self.norm(x))) # Pre-Norm\n",
    "        else:\n",
    "            return self.norm(x + self.dropout(sub_layer(x))) # Post-Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(SubLayer):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        \n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(q, k, v, mask=None, dropout=None):\n",
    "        d_k = q.shape[-1]\n",
    "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill_(mask==0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ v), attention_scores\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x, self.attention_scores = self.attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], value.shape[1], self.h * self.d_k)\n",
    "        return self.w_o(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
