{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d007a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da749f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341bc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    InputEmbeddings class:\n",
    "    - Converts input token indices into dense vectors of dimension `d_model`.\n",
    "    - Scales embeddings by sqrt(d_model) to stabilize gradients in Transformer models.\n",
    "    - Parameters:\n",
    "        - d_model (int): size of each embedding vector.\n",
    "        - num_embeddings (int): size of the vocabulary.\n",
    "    - Usage: feeds into Transformer encoder/decoder as input embeddings.\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length), dtype=torch.long containing token indices.\n",
    "    Outputs:\n",
    "        - embeddings (Tensor): shape (batch_size, seq_length, d_model) scaled embeddings ready for the Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_embeddings: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of Vec\n",
    "        self.num_embeddings = num_embeddings # Size of Vocab\n",
    "        self.embedding = nn.Embedding(num_embeddings, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbf59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding class:\n",
    "    - Adds positional information to token embeddings so the Transformer can capture the order of tokens.\n",
    "    - Uses sine and cosine functions of different frequencies for each dimension.\n",
    "    - Parameters:\n",
    "        - d_model (int): dimension of embedding vectors.\n",
    "        - seq_len (int): maximum sequence length.\n",
    "        - dropout (float): dropout rate applied after adding positional encoding.\n",
    "    - Usage: added to token embeddings before feeding into Transformer layers.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model) token embeddings from InputEmbeddings.\n",
    "    Outputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model) token embeddings with positional information added, dropout applied.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        i = torch.arange(0, d_model, 2, dtype=torch.float)\n",
    "        div_term = torch.exp(i * (-math.log(10000)) / d_model)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :])\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da09f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNormalization class:\n",
    "    - Normalizes inputs across the last dimension to have zero mean and unit variance.\n",
    "    - Learnable parameters (alpha, bias) allow the model to scale and shift the normalized values.\n",
    "    - Helps stabilize and accelerate training of deep networks, especially Transformers.\n",
    "    - Parameters:\n",
    "        - eps (float): small value to avoid division by zero (default 1e-6).\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (..., features), can be any shape with the last dim as features.\n",
    "    Outputs:\n",
    "        - normalized_x (Tensor): same shape as input, normalized along the last dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55fece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SubLayer(nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    SubLayer abstract class:\n",
    "    - Base class for Transformer sub-layers (e.g., attention, feed-forward).\n",
    "    - Enforces implementation of the `forward` method in all subclasses.\n",
    "    - Can accept additional keyword arguments for flexibility.\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input embeddings or outputs from previous layer.\n",
    "        - **kwargs: optional additional arguments needed by specific sub-layers.\n",
    "    Outputs:\n",
    "        - Tensor: processed output, same shape as input in most cases.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def forward(self, x, **kwargs):\n",
    "        raise NotImplementedError(\"Subclasses must implement the forward method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(SubLayer):\n",
    "    \"\"\"\n",
    "    FeedForward sub-layer for Transformers:\n",
    "    - Implements a 2-layer position-wise feed-forward network.\n",
    "    - Applies ReLU activation and dropout between the two linear layers.\n",
    "    - Expands and then projects back to `d_model` dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "        - d_model (int): input and output dimension of the sub-layer.\n",
    "        - d_ff (int): hidden layer dimension (usually larger than d_model).\n",
    "        - dropout (float): dropout probability applied after the activation.\n",
    "    \n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input from previous layer.\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), transformed output.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279dbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    ResidualConnection module for Transformers:\n",
    "    - Wraps a sub-layer (e.g., FeedForward or Attention) with a residual connection and layer normalization.\n",
    "    - Supports both Pre-Norm and Post-Norm variants:\n",
    "        * Pre-Norm: normalization before the sub-layer.\n",
    "        * Post-Norm: normalization after adding the sub-layer output.\n",
    "    - Applies dropout to the sub-layer output before adding the residual.\n",
    "\n",
    "    Parameters:\n",
    "        - dropout (float): dropout probability applied to sub-layer output.\n",
    "        - pre_norm (bool): if True, use Pre-Norm; else, use Post-Norm.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input to the residual block.\n",
    "        - sub_layer (SubLayer): a Transformer sub-layer implementing the forward(x) method.\n",
    "\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), output after residual addition and normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout: float, pre_norm: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.pre_norm = pre_norm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sub_layer: SubLayer):\n",
    "        if self.pre_norm:\n",
    "            return x + self.dropout(sub_layer(self.norm(x))) # Pre-Norm\n",
    "        else:\n",
    "            return self.norm(x + self.dropout(sub_layer(x))) # Post-Norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(SubLayer):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module for Transformers:\n",
    "    - Computes attention over queries, keys, and values split across multiple heads.\n",
    "    - Each head learns to focus on different parts of the input sequence.\n",
    "    - Supports optional attention masking and dropout.\n",
    "\n",
    "    Parameters:\n",
    "        - d_model (int): dimension of input embeddings.\n",
    "        - h (int): number of attention heads.\n",
    "        - dropout (float): dropout probability applied to attention scores.\n",
    "\n",
    "    Inputs:\n",
    "        - q (Tensor): shape (batch_size, seq_length, d_model), query embeddings.\n",
    "        - k (Tensor): shape (batch_size, seq_length, d_model), key embeddings.\n",
    "        - v (Tensor): shape (batch_size, seq_length, d_model), value embeddings.\n",
    "        - mask (Tensor, optional): shape broadcast-able to (batch_size, h, seq_length, seq_length), \n",
    "        used to mask out positions (e.g., for padding or causal attention).\n",
    "\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), output after multi-head attention.\n",
    "        - attention_scores (Tensor): shape (batch_size, h, seq_length, seq_length), \n",
    "        attention weights for each head.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        \n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(q, k, v, mask=None, dropout=None):\n",
    "        d_k = q.shape[-1]\n",
    "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill_(mask==0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ v), attention_scores\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x, self.attention_scores = self.attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], value.shape[1], self.h * self.d_k)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2872a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    EncoderBlock module for Transformers:\n",
    "    - Combines a Multi-Head Self-Attention layer and a Feed-Forward layer.\n",
    "    - Each sub-layer is wrapped with a residual connection and layer normalization.\n",
    "    - Self-Attention supports optional masking (e.g., padding or causal attention).\n",
    "\n",
    "    Parameters:\n",
    "        - self_attention_block (MultiHeadAttention): multi-head attention sub-layer.\n",
    "        - feedforward_block (FeedForward): feed-forward sub-layer.\n",
    "        - dropout (float): dropout probability applied to sub-layer outputs.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input embeddings.\n",
    "        - source_mask (Tensor, optional): shape broadcast-able to (batch_size, h, seq_length, seq_length), \n",
    "        used to mask out certain positions in attention.\n",
    "\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), output after attention and feed-forward layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, self_attention_block: MultiHeadAttention, feedforward_block: FeedForward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feedforward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x, source_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, source_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf69523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module for Transformers:\n",
    "    - Stacks multiple EncoderBlocks to form the full encoder.\n",
    "    - Each block contains self-attention and feed-forward layers with residual connections and layer normalization.\n",
    "    - Optional final LayerNormalization applied after all blocks (Post-Norm).\n",
    "\n",
    "    Parameters:\n",
    "        - layers (nn.ModuleList): list of EncoderBlock instances to be applied sequentially.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, seq_length, d_model), input embeddings.\n",
    "        - mask (Tensor, optional): shape broadcast-able to (batch_size, seq_length, seq_length),\n",
    "        used to mask out certain positions in self-attention (e.g., padding tokens or causal attention).\n",
    "\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, seq_length, d_model), encoded representations after passing through all blocks\n",
    "        and post-layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) # Post-Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Decoder block for Transformers:\n",
    "    - Consists of self-attention, cross-attention, and feed-forward layers.\n",
    "    - Each layer uses residual connections with dropout and layer normalization.\n",
    "    - Self-attention prevents attending to future tokens using a target mask.\n",
    "    - Cross-attention attends over encoder outputs with an optional source mask to ignore padding.\n",
    "\n",
    "    Parameters:\n",
    "        - self_attention_block (MultiHeadAttention): multi-head self-attention for the target sequence.\n",
    "        - cross_attention_block (MultiHeadAttention): multi-head attention to attend to encoder outputs.\n",
    "        - feed_forward_block (FeedForward): position-wise feed-forward network.\n",
    "        - dropout (float): dropout rate applied in residual connections.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): shape (batch_size, target_len, d_model), input embeddings or previous decoder output.\n",
    "        - encoder_output (Tensor): shape (batch_size, source_len, d_model), output of the encoder.\n",
    "        - source_mask (Tensor, optional): shape broadcast-able to (batch_size, 1, 1, source_len),\n",
    "        used to mask out padding tokens in the source sequence.\n",
    "        - target_mask (Tensor, optional): shape broadcast-able to (batch_size, 1, target_len, target_len),\n",
    "        used to mask future tokens in self-attention.\n",
    "\n",
    "    Outputs:\n",
    "        - Tensor: shape (batch_size, target_len, d_model), output of the decoder block after\n",
    "        self-attention, cross-attention, and feed-forward layers with residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        self_attention_block: MultiHeadAttention,\n",
    "        cross_attention_block: MultiHeadAttention,\n",
    "        feed_forward_block: FeedForward,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "        \n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, source_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a41738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder:\n",
    "    - Stack of DecoderBlocks with masked self-attention, cross-attention, and feed-forward layers.\n",
    "    - Applies final LayerNorm after all blocks.\n",
    "\n",
    "    Parameters:\n",
    "        - layers (nn.ModuleList): list of DecoderBlock modules.\n",
    "\n",
    "    Inputs:\n",
    "        - x (Tensor): (batch_size, target_len, d_model), input embeddings or previous decoder output.\n",
    "        - encoder_output (Tensor): (batch_size, source_len, d_model), encoder outputs.\n",
    "        - source_mask (Tensor, optional): masks padding in encoder sequence.\n",
    "        - target_mask (Tensor, optional): masks future tokens in target sequence.\n",
    "\n",
    "    Output:\n",
    "        - Tensor: (batch_size, target_len, d_model), final decoder representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, encoder_output, source_mask, target_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, source_mask, target_mask)\n",
    "        return self.norm(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
